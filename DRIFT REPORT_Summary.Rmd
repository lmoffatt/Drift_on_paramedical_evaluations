---
title: "DRIFT on paramedical evaluations"
author: "Luciano Moffatt"
date: "19/5/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(rstan)
options(mc.cores=parallel::detectCores());
```
```{r}
logit<-function(p){ log(p/(1-p))}
invlogit<-function(a){ 1./(1.+exp(-a))}

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:



# Project details



In the domain of paramedicine students typically have their simulation scenarios evaluated by using a Global Rating Scale (GRS) that has been validated by Tavares and colleagues (Tavers et al., 2013). The GRS is a seven-dimension scale that is used to assess paramedic student’s competence as it pertains to: situational awareness, history gathering, patient assessment, decision making, resource utilization, communication, and procedural skills (Tavers et al., 2013). Each of the seven themes listed above are measured on a scale that ranges from 1 (unsafe) to 7 (exceptional) (Tavers et al., 2014). It has been established that the GRS is valid and possesses high inter-rater reliability when it is used to evaluate paramedics in training, additionally, there is evidence to suggest that values achieved on a GRS in simulation are transferable to values that are attained in a real clinical context (Tavers et al., 2014). 

[comment]
so, we have a human rating scale (1-7) where an instructor evaluates students competence on 7 dimensions.

[\\comment]


Although the GRS has been proven to be the gold standard when it comes to paramedic student evaluation, to the best of knowledge, no one has explored the effects that differential rater function over time (DRIFT) has on the outcomes of the grades obtained on the GRS. The concept of DRIFT has been demonstrated in other areas of education and is typically a result in increasing leniency due to rater fatigue (McLaughlin et al., 2009). Fairness in assessment is crucial to education, additionally, in a domain such as paramedicine it is important that standardization occurs in evaluation as public health and safety could be compromised if this is not the case (Yeates et al., 2019). 

[comment]
so, the objective is to assertain if a differential rater function over time (DRIFT) exists
so, if the rater changes its internal scale with the succesive evaluations. 
[\\comment]



As it is crucial for student success and public safety to ensure that evolutions of paramedic performance are accurate, the primary purpose of this study is to explore if rater fatigue contributes to DRIFT during multiple GRS evaluations for the GRS raters. 

Methods 

The research was approved by the Collège Boréal Research Ethics Board. Following ethical approval consent was acquired from participants post-practical examinations. This study examined rater evaluations during a practical paramedic student evaluation using the GRS that compromised ______stations lasting ___minutes. 
The raters were all currently employed paramedics who were in possession of the Advanced Emergency Medical Care Assistant (AEMCA) qualification and legally allowed to work as a paramedic in the province of Ontario, Canada. Additionally, the raters were staff at Collège Boréal (Collège Boréal, Sudbury, Ontario, Canada). Each of the raters involved in the study were familiar with the GRS evaluation tool and had previously been involved in their evaluations before. The examinees were first- and second-year paramedic students that were enrolled in the college and were participating in the curriculum required practical assessments that take place three times per semester. 

The student examinees were randomly assigned to start at a different station and moved through the GRS circuit in the same order. Each rater scored each paramedic student examinee at the same station and was given a single rest period during the practical examination that was staggered throughout. 

In order to estimate the difficulty for each station we used the Prehospital Canadian Triage and Acuity Scale (CTAS) as outlined in the Paramedic Guide Version 2.0 that is published by the Emergency Health Services Branch from the Ministry of Health and Long-Term Care in Ontario, Canada. The CTAS scale is a 1-5 scale with level one being resuscitation, level two being emergent, level 3 urgent, level 4, less urgent, and level 5-non urgent. We presumed that the more urgent the situation the more complex the simulated scenario. 



```{r warning=FALSE}

db  <- read_excel("DRIFT REPORT Semster 1 and 2 _July 14 Luciano i.xlsx", 
    col_types = c("text", "numeric", "numeric", 
        "numeric", "numeric", "date", "text", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "text", "numeric", "numeric", 
        "numeric"), skip = 10)
db%>%filter(!is.na(Student))->db2

db2$Time=db2$Time*24

colnames(db2)[1]<-"Evaluator"

pivot_longer(db2,cols=c("ET","CD","EP","PD","UR","CM","DT"),
             names_to="scoreDim",values_to = "score")->d3

d3%>%filter(!is.na(score))->d

d$Time[d$Time<8]=d$Time[d$Time<8]+12

d$h=floor(d$Time/2)*2




```


# Modelling the scores for testing the Drift

In order to analyze the possibility of a drift we first have to find an statistical model that describes the probability of a student to achieve a given score at a particular task. 

A first approximation would be that the score correspond to the successful completion of 6 independent cumulative tasks, each one with the same probability of completion. 

This is described by a Binomial distribution of Bi(6,p). 


We considered that the parameter p depends on several factors:

1. The Student, different students might score different in average
2. The Evaluator, different evaluators might evaluate differently
3. The dimension of the evaluation, some dimensions might be more difficult
4. The category of the task, some tasks might be more difficult. 
5. The date. Students might perform better (or worse). 

and we consider the possibility of some interactions: 
1. Students might vary in the dimension they perform better. 
2. Students might vary in the category they perform better. 


We designed models that considered different factors and we evaluated the Evidence of each one. 

```{r}


Evidence_from_file<-function(abbr)
{
   str_c("DRIFT REPORT_",abbr,".RData")->s
   load(s)
   E2$I$Ev
   
}
```


```{r}
tibble(abbr=c("","S","SE","SEC","SED","SECDY","SEDs"),
       model=c("all equal","Student",
              "Student+Evaluator",
              "Student+Evaluator+Category",
              "Student+Evaluator+Dimension",
              "Student+Evaluator+Dimension+DateOfYear",
              "Student+Evaluator+Dimension x Student"
              )
       
       )->t

t%>%mutate(Evidence=map_dbl(abbr,Evidence_from_file))->t
t$K=(t$Evidence-t$Evidence[5])/log(10)*10
```


```{r}
print(t)
              
              

```

The evidence is in natural logarithm units, so a difference grater than 2.3 translate into a Bayes factor K greater than 10, which indicates a "strong" strength of evidence. 
The model SED, which allow for each Student, each Evaluator and each Dimension to affect the score is the one with the strongest Evidence, so we will use it as a base to study the possibility of a perceptual drift in the evaluators. 

## Modelling DRIFT

We consider two mechanisms for fatigue to affect the evaulators. 

1. Leniency drift model. The fatigue generated by successive evaluations translate itself into a systematic change in the leniency/strictness evaluations, for the same performace the score would change over time. 

2. Perceptual failure model. The fatigue results in an increase in the error rate at the evaluations. We consider two probabilities: 

   alpha: (type I error) the evaluator considers that the student failed in a particular objective given that the student succeeded. 
   
   beta: (type II error) the evaluator considers that the student succeeded given that the student actually failed. 

In the model we consider that the error rate increases linearly with the number of test performed. 


So the table for the Drif models is

```{r}
tibble(abbr=c("SED","SED_Leniancy","SED_Perceptual"),
       model=c("Student+Evaluator+Dimension",
          "SED+Leniancy",
              "SED+Perceptual")
       
       )->t2

t2%>%mutate(Evidence=map_dbl(abbr,Evidence_from_file))->t2
t2$E0=t2$Evidence-t2$Evidence[1]
t2$K=(t2$E0)/log(10)*10
print(t2)
```

So, there is "substantial" (10^0.5>1/K>10) evidence against either Leniency and Perceptual models of Drift. 
That means that the data provided substantial evidence favoring a model where there is no Drift in comparison to these two models that considers this possibility. 

However, it might be the case that tiny changes in the evaluations exists. The bayesian analysis set a credibility distribution for those rate of changes. 

## Leniancy model

In this model the probability of succeeding each one of the 6 stages changes with the following equation

$$ logit(p)=logit(p_0)+ b\cdot n_{test}
$$

```{r}



Data_from_file<-function(abbr)
{
   str_c("DRIFT REPORT_",abbr,".RData")->s
   load(s)
   as.data.frame(r2[[length(beta)]])->mcmc2_1;
   mcmc2_1$beta=1;
   as.data.frame(r2[[1]])->mcmc2_0;
   mcmc2_0$beta=0;

   rbind(mcmc2_0,mcmc2_1)->mcmc2;
   mcmc2$distribution=c("prior","posterior")[mcmc2$beta+1];
   mcmc2$model=abbr;
   mcmc2
}
```


```{r}
d<-Data_from_file("SED_Leniancy")
```


```{r}
d%>%select(distribution,sn_mean__d_logit_score__d_ntest_prior,
           mean_d_logit_score__d_ntest_prior
  )->d2;


```


```{r}
ggplot(d2,aes(x=mean_d_logit_score__d_ntest_prior*10, after_stat(ndensity),group=distribution))+
   geom_density(aes(fill=distribution),alpha=0.2)+
   theme(axis.title.y=element_blank(),
         axis.text.y=element_blank(),
         axis.ticks.y=element_blank()
         )+xlab("increase in logit(pscore) after 10 tests [logit/#tests]")

```


The data indicates a huge reduction in the range of values. 

A credibility boudary indicates 

```{r}
quantile((filter(d2,distribution=="posterior"))$mean_d_logit_score__d_ntest_prior*10,
         probs = c(0.01,0.025,0.975,0.99))->q
q
```

In score terms, starting with a score of 4, the score after 10 tests would be
```{r}
invlogit(q)*6+1
```


So, the data set a limit on the credible drift: it would be less than 0.3 points. 

## Perceptual model

The perceptual model assumes two parameters

$\alpha$-> $\Pr(Evaluator \;dissaprove|Student\;succeeded)$
$\beta$-> $\Pr(Evaluator \;aprove|Student\;failed)$

The logit transformation of both is assumed to increase linearly with the number of tests. 


```{r}
dp<-Data_from_file("SED_Perceptual")
```

```{r}
dp%>%select(distribution,mean_alpha_drift,mean_beta_drift
  )->dp2;


```


```{r}
ggplot(dp2,aes(x=mean_alpha_drift*10/(mean_alpha_drift*10+1), after_stat(ndensity),group=distribution))+
   geom_density(aes(fill=distribution),alpha=0.2)+scale_x_log10()+
   xlab("alpha after 10 tests")+
   theme(axis.title.y=element_blank(),
         axis.text.y=element_blank(),
         axis.ticks.y=element_blank()
         )

```

```{r}
ggplot(dp2,aes(x=mean_beta_drift*10/(mean_beta_drift*10+1), after_stat(ndensity),group=distribution))+
   geom_density(aes(fill=distribution),alpha=0.2)+scale_x_log10()+
   xlab("beta after 10 tests")+
   theme(axis.title.y=element_blank(),
         axis.text.y=element_blank(),
         axis.ticks.y=element_blank()
         )

```



The credibility interval for them are
```{r}
dp2%>%filter(distribution=="posterior")->dp3
rbind(quantile(dp3$mean_alpha_drift*10/(dp3$mean_alpha_drift*10+1),
               c(0.01,0.025,0.975,0.99)),
      quantile(dp3$mean_beta_drift*10/(dp3$mean_beta_drift*10+1),
               c(0.01,0.025,0.975,0.99))
      )->q2
row.names(q2)=c("alpha after 10 tests","beta after 10 tests")
print(q2)
```

So both alpha and beta are less than 0.2 after 10 tests. 
Lets consider what would happen for p=5/6 and p=1/6

```{r}
alpha_10=dp3$mean_alpha_drift*10/(dp3$mean_alpha_drift*10+1)
beta_10=dp3$mean_beta_drift*10/(dp3$mean_beta_drift*10+1)

p_6_0=(5/6*(1-dp3$mean_alpha_drift)+1/6*dp3$mean_beta_drift)*6+1
p_1_0=(1/6*(1-dp3$mean_alpha_drift)+5/6*dp3$mean_beta_drift)*6+1

p_6_10=(5/6*(1-alpha_10)+1/6*beta_10)*6+1
p_1_10=(1/6*(1-alpha_10)+5/6*beta_10)*6+1


rbind(quantile(p_6_0,c(0.01,0.025,0.975,0.99)),
      quantile(p_1_0,c(0.01,0.025,0.975,0.99)),
      quantile(p_6_10,c(0.01,0.025,0.975,0.99)),
      quantile(p_1_10,c(0.01,0.025,0.975,0.99))
      )->q3
row.names(q3)=c("p=5/6 at 1 test","p=1/6 at 1 test",
                "p=5/6 after 10 tests",
                "p=1/6 after 10 tests")
print(q3)
```

So, the model indicate that the data is compatible with an increase of  up to 0.8 points for bad scores and diminish up to 0.5 points for very good scores after 10 tests. 



# Conclusion

1. Bayesian modelling favors a model where the only factors affecting the score are the identity of the Student, the identity of the Evaluator and the dimension of the evaluation. 

2. Bayesian modelling provide evidence against a fatigue effect of repeated testing for the Evaluators. 

3. In the case we have strong reason to believe that the DRIFT effect do take place, we can set a maximum amount of change in the scoring after 10 tests. 
If we believe that fatigue just changes the scores homogeneously, we expect a difference of no more than 0.2 after 10 tests. 

4. If we believe that fatigue generate appreciation errors where failures are accepted and success is rejected, we found that at the most a decrease of 0.8 points for good scores and an increase of 0.5 for bad scores is compatible with the data. 

5. Please, remember, those are limits; the data argues against the presence of a DRIFT effect. 


